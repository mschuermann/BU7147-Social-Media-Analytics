CTM       = CTM(s_dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),
VEM       = LDA(s_dtm, k = k, control = list(seed = SEED)),
VEM_Fixed = LDA(s_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
Gibbs     = LDA(s_dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000,
thin = 100,    iter = 1000))
)
models
# Top 10 terms of each topic for each model
# Do you see any themes you can label to these "topics" (lists of words)?
lapply(models, terms, 10)
# This might take a minute!
s_models <- list(
CTM       = CTM(s_dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),
VEM       = LDA(s_dtm, k = k, control = list(seed = SEED)),
VEM_Fixed = LDA(s_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
Gibbs     = LDA(s_dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000,
thin = 100,    iter = 1000))
)
# Remove the stems associated with our search terms!
corpus <- tm_map(corpus, removeWords, c("samsung", "galaxy", "ultra"), mc.cores=1)
s_doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(s_corpus)))
s_dtm <- DocumentTermMatrix(s_corpus)
# Now for some topics
SEED = sample(1:1000000, 1)  # Pick a random seed for replication
k = 10  # Let's start with 10 topics
# This might take a minute!
s_models <- list(
CTM       = CTM(s_dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),
VEM       = LDA(s_dtm, k = k, control = list(seed = SEED)),
VEM_Fixed = LDA(s_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
Gibbs     = LDA(s_dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000,
thin = 100,    iter = 1000))
)
# Top 10 terms of each topic for each model
# Do you see any themes you can label to these "topics" (lists of words)?
lapply(s_models, terms, 10)
# Remove the stems associated with our search terms!
corpus <- tm_map(corpus, removeWords, c("samsung", "galaxy", "ultra", "\"samsung", "\"galaxy"), mc.cores=1)
# Remove the stems associated with our search terms!
s_corpus <- tm_map(corpus, removeWords, c("samsung", "galaxy", "ultra", "\"samsung", "\"galaxy"), mc.cores=1)
s_doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(s_corpus)))
s_dtm <- DocumentTermMatrix(s_corpus)
# Now for some topics
SEED = sample(1:1000000, 1)  # Pick a random seed for replication
k = 10  # Let's start with 10 topics
# This might take a minute!
s_models <- list(
CTM       = CTM(s_dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),
VEM       = LDA(s_dtm, k = k, control = list(seed = SEED)),
VEM_Fixed = LDA(s_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
Gibbs     = LDA(s_dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000,
thin = 100,    iter = 1000))
)
# Top 10 terms of each topic for each model
# Do you see any themes you can label to these "topics" (lists of words)?
lapply(s_models, terms, 10)
##### LDA Topic Modelling #####
# Create corpus object
s_corpus <- Corpus(VectorSource(bg_df_s))
# Remove English stop words.
s_corpus <- tm_map(s_corpus, removeWords, stopwords("en"), mc.cores=1)
# Remove numbers.
s_corpus <- tm_map(s_corpus, removeNumbers, mc.cores=1)
# Stem the words.
s_corpus <- tm_map(s_corpus, stemDocument, mc.cores=1)
# Remove the stems associated with our search terms!
s_corpus <- tm_map(corpus, removeWords, c("samsung", "galaxy", "ultra", "\"samsung", "\"galaxy"), mc.cores=1)
s_corpus
s_doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(s_corpus)))
s_dtm <- DocumentTermMatrix(s_corpus)
# Now for some topics
SEED = sample(1:1000000, 1)  # Pick a random seed for replication
k = 10  # Let's start with 10 topics
# This might take a minute!
s_models <- list(
CTM       = CTM(s_dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),
VEM       = LDA(s_dtm, k = k, control = list(seed = SEED)),
VEM_Fixed = LDA(s_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
Gibbs     = LDA(s_dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000,
thin = 100,    iter = 1000))
)
# Top 10 terms of each topic for each model
# Do you see any themes you can label to these "topics" (lists of words)?
lapply(s_models, terms, 10)
s_doc.lengths
options(mc.cores = 10)
tm_parLapply_engine(parallel::mclapply)  # mclapply gets the number of cores from global options
tm_parLapply_engine(parallel::mclapply)  # mclapply gets the number of cores from global options
# Create corpus object
s_corpus <- Corpus(VectorSource(bg_df_s))
# Remove English stop words.
s_corpus <- tm_map(s_corpus, removeWords, stopwords("en"), mc.cores=1)
# Remove English stop words.
s_corpus <- tm_map(s_corpus, removeWords, stopwords("en"), mc.cores=10)
# Remove English stop words.
s_corpus <- tm_map(s_corpus, removeWords, stopwords("en"), mc.cores=1)
# Remove English stop words.
s_corpus <- tm_map(s_corpus, removeWords, stopwords("en"))
# Remove numbers.
s_corpus <- tm_map(s_corpus, removeNumbers)
# Stem the words.
s_corpus <- tm_map(s_corpus, stemDocument)
# Remove the stems associated with our search terms!
s_corpus <- tm_map(corpus, removeWords, c("samsung", "galaxy", "ultra", "\"samsung", "\"galaxy"))
s_doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(s_corpus)))
s_doc.lengths
s_dtm <- DocumentTermMatrix(s_corpus)
# Now for some topics
SEED = sample(1:1000000, 1)  # Pick a random seed for replication
k = 10  # Let's start with 10 topics
# This might take a minute!
s_models <- list(
CTM       = CTM(s_dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),
VEM       = LDA(s_dtm, k = k, control = list(seed = SEED)),
VEM_Fixed = LDA(s_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
Gibbs     = LDA(s_dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000,
thin = 100,    iter = 1000))
)
# Top 10 terms of each topic for each model
# Do you see any themes you can label to these "topics" (lists of words)?
lapply(s_models, terms, 10)
# Top 10 terms of each topic for each model
# Do you see any themes you can label to these "topics" (lists of words)?
lapply(s_models, terms, 10)
s_assignments <- sapply(s_models, topics)
head(s_assignments, n=10)
s_assignments
k = 5 # Let's start with 10 topics
# This might take a minute!
s_models <- list(
CTM       = CTM(s_dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),
VEM       = LDA(s_dtm, k = k, control = list(seed = SEED)),
VEM_Fixed = LDA(s_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
Gibbs     = LDA(s_dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000,
thin = 100,    iter = 1000))
)
# Top 10 terms of each topic for each model
# Do you see any themes you can label to these "topics" (lists of words)?
lapply(s_models, terms, 5)
s_assignments <- sapply(s_models, topics)
s_assignments
##### LDA Topic Modelling #####
options(mc.cores = 10)
tm_parLapply_engine(parallel::mclapply)  # mclapply gets the number of cores from global options
# Create corpus object
a_corpus <- Corpus(VectorSource(bg_df_a))
# Remove English stop words.
a_corpus <- tm_map(a_corpus, removeWords, stopwords("en"))
# Remove numbers.
a_corpus <- tm_map(a_corpus, removeNumbers)
# Stem the words.
a_corpus <- tm_map(a_corpus, stemDocument)
# Remove the stems associated with our search terms!
a_corpus <- tm_map(corpus, removeWords, c("samsung", "galaxy", "ultra", "\"samsung", "\"galaxy"))
a_doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(a_corpus)))
a_dtm <- DocumentTermMatrix(a_corpus)
# Now for some topics
SEED = sample(1:1000000, 1)  # Pick a random seed for replication
k = 5
# This might take a minute!
a_models <- list(
CTM       = CTM(a_dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),
VEM       = LDA(a_dtm, k = k, control = list(seed = SEED)),
VEM_Fixed = LDA(a_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
Gibbs     = LDA(a_dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000,
thin = 100,    iter = 1000))
)
# There you have it. Models now holds 4 topics. See the topicmodels API documentation for details
# Top 10 terms of each topic for each model
# Do you see any themes you can label to these "topics" (lists of words)?
lapply(a_models, terms, 5)
a_assignments <- sapply(a_models, topics)
a_assignments
# Remove the stems associated with our search terms!
a_corpus <- tm_map(corpus, removeWords, c("iphone"))
a_doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(a_corpus)))
a_dtm <- DocumentTermMatrix(a_corpus)
# Now for some topics
SEED = sample(1:1000000, 1)  # Pick a random seed for replication
k = 5
# This might take a minute!
a_models <- list(
CTM       = CTM(a_dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),
VEM       = LDA(a_dtm, k = k, control = list(seed = SEED)),
VEM_Fixed = LDA(a_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
Gibbs     = LDA(a_dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000,
thin = 100,    iter = 1000))
)
# Top 10 terms of each topic for each model
# Do you see any themes you can label to these "topics" (lists of words)?
lapply(a_models, terms, 5)
a_assignments <- sapply(a_models, topics)
a_assignments
# Remove the stems associated with our search terms!
a_corpus <- tm_map(a_corpus, removeWords, c("samsung", "galaxy", "ultra", "\"samsung", "\"galaxy"))
a_doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(a_corpus)))
a_dtm <- DocumentTermMatrix(a_corpus)
# Now for some topics
SEED = sample(1:1000000, 1)  # Pick a random seed for replication
k = 5
# This might take a minute!
a_models <- list(
CTM       = CTM(a_dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),
VEM       = LDA(a_dtm, k = k, control = list(seed = SEED)),
VEM_Fixed = LDA(a_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
Gibbs     = LDA(a_dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000,
thin = 100,    iter = 1000))
)
# Top 10 terms of each topic for each model
# Do you see any themes you can label to these "topics" (lists of words)?
lapply(a_models, terms, 5)
# Remove the stems associated with our search terms!
a_corpus <- tm_map(a_corpus, removeWords, c("iphone"))
a_doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(a_corpus)))
a_dtm <- DocumentTermMatrix(a_corpus)
# Now for some topics
SEED = sample(1:1000000, 1)  # Pick a random seed for replication
k = 5
# This might take a minute!
a_models <- list(
CTM       = CTM(a_dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),
VEM       = LDA(a_dtm, k = k, control = list(seed = SEED)),
VEM_Fixed = LDA(a_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
Gibbs     = LDA(a_dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000,
thin = 100,    iter = 1000))
)
# Top 10 terms of each topic for each model
# Do you see any themes you can label to these "topics" (lists of words)?
lapply(a_models, terms, 5)
a_assignments <- sapply(a_models, topics)
# Remove the stems associated with our search terms!
s_corpus <- tm_map(s_corpus, removeWords, c("samsung", "galaxy", "ultra", "\"samsung", "\"galaxy"))
s_doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(s_corpus)))
s_dtm <- DocumentTermMatrix(s_corpus)
# Now for some topics
SEED = sample(1:1000000, 1)  # Pick a random seed for replication
k = 5
# This might take a minute!
s_models <- list(
CTM       = CTM(s_dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),
VEM       = LDA(s_dtm, k = k, control = list(seed = SEED)),
VEM_Fixed = LDA(s_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
Gibbs     = LDA(s_dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000,
thin = 100,    iter = 1000))
)
# Top 10 terms of each topic for each model
# Do you see any themes you can label to these "topics" (lists of words)?
lapply(s_models, terms, 5)
s_assignments <- sapply(s_models, topics)
s_assignments
bg_df_a
# Create corpus object
a_corpus <- Corpus(VectorSource(bg_df_a))
# Create corpus object
a_corpus <- Corpus(VectorSource(bg_df_a))
# Remove English stop words.
a_corpus <- tm_map(a_corpus, removeWords, stopwords("en"))
# Remove numbers.
a_corpus <- tm_map(a_corpus, removeNumbers)
# Stem the words.
a_corpus <- tm_map(a_corpus, stemDocument)
# Remove the stems associated with our search terms!
a_corpus <- tm_map(a_corpus, removeWords, c("iphone"))
a_doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(a_corpus)))
a_dtm <- DocumentTermMatrix(a_corpus)
# Now for some topics
SEED = sample(1:1000000, 1)  # Pick a random seed for replication
k = 5
# This might take a minute!
a_models <- list(
CTM       = CTM(a_dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),
VEM       = LDA(a_dtm, k = k, control = list(seed = SEED)),
VEM_Fixed = LDA(a_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
Gibbs     = LDA(a_dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000,
thin = 100,    iter = 1000))
)
# Top 10 terms of each topic for each model
# Do you see any themes you can label to these "topics" (lists of words)?
lapply(a_models, terms, 5)
# Remove the stems associated with our search terms!
a_corpus <- tm_map(a_corpus, removeWords, c("iphone", "iphon", "\"iphon"))
a_doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(a_corpus)))
a_dtm <- DocumentTermMatrix(a_corpus)
# Now for some topics
SEED = sample(1:1000000, 1)  # Pick a random seed for replication
k = 5
# This might take a minute!
a_models <- list(
CTM       = CTM(a_dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),
VEM       = LDA(a_dtm, k = k, control = list(seed = SEED)),
VEM_Fixed = LDA(a_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
Gibbs     = LDA(a_dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000,
thin = 100,    iter = 1000))
)
# Top 10 terms of each topic for each model
# Do you see any themes you can label to these "topics" (lists of words)?
lapply(a_models, terms, 5)
a_assignments <- sapply(a_models, topics)
# Remove the stems associated with our search terms!
a_corpus <- tm_map(a_corpus, removeWords, c("iphone", "iphon", "\"iphon", "appl"))
a_doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(a_corpus)))
a_dtm <- DocumentTermMatrix(a_corpus)
# Now for some topics
SEED = sample(1:1000000, 1)  # Pick a random seed for replication
k = 5
# This might take a minute!
a_models <- list(
CTM       = CTM(a_dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),
VEM       = LDA(a_dtm, k = k, control = list(seed = SEED)),
VEM_Fixed = LDA(a_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
Gibbs     = LDA(a_dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000,
thin = 100,    iter = 1000))
)
# Top 10 terms of each topic for each model
# Do you see any themes you can label to these "topics" (lists of words)?
lapply(a_models, terms, 5)
a_assignments <- sapply(a_models, topics)
# Remove the stems associated with our search terms!
a_corpus <- tm_map(a_corpus, removeWords, c("iphone", "iphon", "\"iphon", "appl", "pro", "max"))
a_doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(a_corpus)))
a_dtm <- DocumentTermMatrix(a_corpus)
# Now for some topics
SEED = sample(1:1000000, 1)  # Pick a random seed for replication
k = 5
# This might take a minute!
a_models <- list(
CTM       = CTM(a_dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),
VEM       = LDA(a_dtm, k = k, control = list(seed = SEED)),
VEM_Fixed = LDA(a_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
Gibbs     = LDA(a_dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000,
thin = 100,    iter = 1000))
)
# Top 10 terms of each topic for each model
# Do you see any themes you can label to these "topics" (lists of words)?
lapply(a_models, terms, 5)
a_assignments <- sapply(a_models, topics)
a_assignments
# Top 10 terms of each topic for each model
# Do you see any themes you can label to these "topics" (lists of words)?
lapply(a_models, terms, 10)
a_assignments <- sapply(a_models, topics)
a_assignments
# Top 10 terms of each topic for each model
# Do you see any themes you can label to these "topics" (lists of words)?
lapply(s_models, terms, 10)
s_assignments <- sapply(s_models, topics)
a_assignments
# Remove the stems associated with our search terms!
a_corpus <- tm_map(a_corpus, removeWords, c("samsung", "galaxy", "ultra", "\"samsung", "\"galaxy", "iphone", "iphon", "\"iphon", "appl", "pro", "max"))
a_doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(a_corpus)))
a_dtm <- DocumentTermMatrix(a_corpus)
# Now for some topics
SEED = sample(1:1000000, 1)  # Pick a random seed for replication
k = 5
# This might take a minute!
a_models <- list(
CTM       = CTM(a_dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),
VEM       = LDA(a_dtm, k = k, control = list(seed = SEED)),
VEM_Fixed = LDA(a_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
Gibbs     = LDA(a_dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000,
thin = 100,    iter = 1000))
)
# Top 10 terms of each topic for each model
# Do you see any themes you can label to these "topics" (lists of words)?
lapply(a_models, terms, 10)
a_assignments <- sapply(a_models, topics)
# Remove the stems associated with our search terms!
a_corpus <- tm_map(a_corpus, removeWords, c("galaxi", "samsung", "galaxy", "ultra", "\"samsung", "\"galaxy", "iphone", "iphon", "\"iphon", "appl", "pro", "max"))
a_doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(a_corpus)))
a_dtm <- DocumentTermMatrix(a_corpus)
# Now for some topics
SEED = sample(1:1000000, 1)  # Pick a random seed for replication
k = 5
# This might take a minute!
a_models <- list(
CTM       = CTM(a_dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),
VEM       = LDA(a_dtm, k = k, control = list(seed = SEED)),
VEM_Fixed = LDA(a_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
Gibbs     = LDA(a_dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000,
thin = 100,    iter = 1000))
)
# Top 10 terms of each topic for each model
# Do you see any themes you can label to these "topics" (lists of words)?
lapply(a_models, terms, 10)
# Remove the stems associated with our search terms!
s_corpus <- tm_map(s_corpus, removeWords, c("galaxi", "samsung", "galaxy", "ultra", "\"samsung", "\"galaxy", "iphone", "iphon", "\"iphon", "appl", "pro", "max"))
s_doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(s_corpus)))
s_dtm <- DocumentTermMatrix(s_corpus)
# Now for some topics
SEED = sample(1:1000000, 1)  # Pick a random seed for replication
k = 5
# This might take a minute!
s_models <- list(
CTM       = CTM(s_dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),
VEM       = LDA(s_dtm, k = k, control = list(seed = SEED)),
VEM_Fixed = LDA(s_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
Gibbs     = LDA(s_dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000,
thin = 100,    iter = 1000))
)
# Top 10 terms of each topic for each model
# Do you see any themes you can label to these "topics" (lists of words)?
lapply(s_models, terms, 10)
s_assignments <- sapply(s_models, topics)
stats_s <- samsung_df %>%
mutate(date = substr(created,1,10))
stats_s <- aggregate(cbind(count_users = screenName, count_tweets = id) ~ date,
data = stats_s,
FUN = function(x){n_distinct(x)})
stats_s <- stats_s %>%
mutate(avg_tweets_per_user = count_tweets / count_users) %>%
mutate(weekday = weekdays(as.Date(date)))
stats_s
stats_a <- aggregate(cbind(count_users = screenName, count_tweets = id) ~ date,
data = stats_a,
FUN = function(x){n_distinct(x)})
#### Apple ####
##### Statistics about dataset #####
stats_a <- apple_df %>%
mutate(date = substr(created,1,10))
stats_a <- aggregate(cbind(count_users = screenName, count_tweets = id) ~ date,
data = stats_a,
FUN = function(x){n_distinct(x)})
stats_a <- stats_a %>%
mutate(avg_tweets_per_user = count_tweets / count_users) %>%
mutate(weekday = weekdays(as.Date(date)))
stats_a
apple_df
View(stats_a)
View(apple_df)
View(apple_df)
#there is another competition which did not need retweets or replying to someone specifically
#people copied the text to take part, therefore, the word "treasure" and "win" were exorbitant in our analysis
#therefore we excluded all rows that had this text
apple_df <- apple_df %>%
filter(!grepl("Join the event to win an iPhone 13!",text))  %>%
filter(!grepl("rolex",text))
write.csv(apple_df,"Apple_df.csv")
#### Apple ####
##### Statistics about dataset #####
stats_a <- apple_df %>%
mutate(date = substr(created,1,10))
stats_a <- aggregate(cbind(count_users = screenName, count_tweets = id) ~ date,
data = stats_a,
FUN = function(x){n_distinct(x)})
stats_a <- stats_a %>%
mutate(avg_tweets_per_user = count_tweets / count_users) %>%
mutate(weekday = weekdays(as.Date(date)))
stats_a
##### Preprocessing Tweets #####
#replace emojis with sentiment
Apple_df <- apple_df$text %>%
str_to_lower() %>% #all text to lower case
replace_contraction() %>% #replaces contractions to longer form
replace_internet_slang() %>% #replaces common internet slang
replace_hash(replacement = "") %>% #removes hashtags
replace_word_elongation() %>% #removes word elongation, e.g. "heeeeey" to "hey"
replace_emoji() %>% #replaces emojis with the word form
replace_emoji_identifier() %>% #replaces emoji identifiers to word form
replace_non_ascii() %>% #replaces common non-ASCII characters.
str_squish() %>% #reduces repeated whitespace inside a string
str_trim() %>% #removes whitespace from start and end of string
{gsub("(RT|via)((?:\\b\\W*@\\w+)+)","",.)} %>% #remove RT (retweets)
{gsub("http[^[:blank:]]+","",.)} %>% #remove links that start with http
{gsub("@\\u+","",.)} %>% #remove names
{gsub('@\\w+', '', .)} %>% # remove at people
{gsub("[[:punct:]]"," ",.)} %>%#remove punctuation
{gsub("[^[:alnum:]]"," ",.)}%>%#remove punctuation
stringr::str_replace_all(stopwords_regex, '') %>% #remove stop words
unique() #remove duplicates
##### sentiment analysis #####
mysentiment_apple<- get_nrc_sentiment(Apple_df)
sentimentscores_apple<- data.frame(colSums(mysentiment_apple[,]))
###### getting sentiment scores ######
names(sentimentscores_apple)<-"score"
sentimentscores_apple<-cbind("sentiment"=rownames(sentimentscores_apple),sentimentscores_apple)
rownames(sentimentscores_apple)<-NULL
###### plotting sentiment scores ######
ggplot(data=sentimentscores_apple,aes(x=sentiment,y=score))+
geom_bar(aes(fill=sentiment),stat="identity")+
theme(legend.position = "none")+
xlab("sentiment") +ylab("score")+ ggtitle("total sentiment score based on tweets about Apple")
###### Sentimentr score #######
sentimentr_apple <- sentiment_by(Apple_df, by=NULL)
ggplot(data=sentimentr_apple,aes(x=element_id,y=ave_sentiment))+
geom_line()
sentimentr_html_a <- sentimentr_apple %>%
sentiment_by(by=NULL)%>%
highlight()
sentiments_a <- get_sentences(Apple_df) %>%
extract_sentiment_terms()
sentimentscores_apple<- data.frame(colSums(mysentiment_apple[,]))
sentimentscores_apple
###### plotting sentiment scores ######
ggplot(data=sentimentscores_apple,aes(x=sentiment,y=score))+
geom_bar(aes(fill=sentiment),stat="identity")+
theme(legend.position = "none")+
xlab("sentiment") +ylab("score")+ ggtitle("total sentiment score based on tweets about Apple")
###### Sentimentr score #######
sentimentr_apple <- sentiment_by(Apple_df, by=NULL)
ggplot(data=sentimentr_apple,aes(x=element_id,y=ave_sentiment))+
geom_line()
sentimentr_html_a <- sentimentr_apple %>%
sentiment_by(by=NULL)%>%
highlight()
sentiments_a <- get_sentences(Apple_df) %>%
extract_sentiment_terms()
sentiments_a
sentiments_a <- get_sentences(Apple_df) %>%
extract_sentiment_terms() %>%
group_by(negative) %>%
summarise_all()
sentiments_a
